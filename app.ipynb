{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30ae2eef-de39-4197-b320-28957a1cd353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py - working version\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- CONFIGURATION & STYLING ---\n",
    "st.set_page_config(page_title=\"ML Classifier Pro\", layout=\"wide\", page_icon=\"ü§ñ\")\n",
    "\n",
    "# Custom CSS for a cleaner look\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    .main { background-color: #f5f7f9; }\n",
    "    .stMetric { background-color: #ffffff; padding: 15px; border-radius: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); }\n",
    "    </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# --- CACHING FOR PERFORMANCE ---\n",
    "@st.cache_resource\n",
    "def load_assets(model_name):\n",
    "    \"\"\"Loads model and scaler once and caches them.\"\"\"\n",
    "    model_path = f\"model/{model_name}.pkl\"\n",
    "    scaler_path = 'model/scaler.pkl'\n",
    "    \n",
    "    if os.path.exists(model_path) and os.path.exists(scaler_path):\n",
    "        return joblib.load(model_path), joblib.load(scaler_path)\n",
    "    return None, None\n",
    "\n",
    "# --- SIDEBAR ---\n",
    "with st.sidebar:\n",
    "    st.header(\"Settings\")\n",
    "    model_options = [\"Logistic_Regression\", \"Decision_Tree\", \"KNN\", \"Naive_Bayes\", \"Random_Forest\", \"XGBoost\"]\n",
    "    selected_model_name = st.selectbox(\"Select Model Architecture\", model_options)\n",
    "    \n",
    "    st.divider()\n",
    "    st.info(\"**Author:** Senthil Kumaran S\\n\\n**BITS ID:** 2024DC04201\")\n",
    "\n",
    "# --- MAIN UI ---\n",
    "st.title(\"ü§ñ Machine Learning Classification Dashboard\")\n",
    "st.markdown(\"Upload your test dataset to evaluate model performance in real-time.\")\n",
    "\n",
    "# Load Model/Scaler\n",
    "model, scaler = load_assets(selected_model_name)\n",
    "\n",
    "if model is None:\n",
    "    st.error(f\"‚ö†Ô∏è Model file for **{selected_model_name}** or scaler.pkl was not found in `/model` directory.\")\n",
    "    st.stop()\n",
    "\n",
    "# File Upload\n",
    "uploaded_file = st.file_uploader(\"Upload your Test Dataset (CSV)\", type=[\"csv\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Try reading with comma, if that results in 1 col, try semicolon\n",
    "    df_raw = pd.read_csv(uploaded_file, sep=\"|\", quoting=3, header=None)\n",
    "    \n",
    "    # 2. Get the content, strip the leading/trailing double quotes\n",
    "    # and split it by the actual comma separator\n",
    "    header_str = df_raw.iloc[0, 0].strip('\"')\n",
    "    data_rows = df_raw.iloc[1:, 0].str.strip('\"')\n",
    "    \n",
    "    # 3. Create the clean DataFrame\n",
    "    data = data_rows.str.split(',', expand=True)\n",
    "    data.columns = header_str.split(',')\n",
    "    \n",
    "    # 4. Clean up column names (remove any lingering quotes)\n",
    "    data.columns = [c.strip('\"') for c in data.columns]\n",
    "      \n",
    "    if data.shape[1] == 1:\n",
    "        uploaded_file.seek(0)\n",
    "        data = pd.read_csv(uploaded_file, sep=';')\n",
    "\n",
    "    st.subheader(\"üìã Data Preview\")\n",
    "    st.dataframe(data.head(5), use_container_width=True)\n",
    "\n",
    "    \n",
    "    # --- PREPROCESSING LOGIC START ---\n",
    "    # Check if data is raw (contains strings like 'blue-collar') or processed (all numeric)\n",
    "    # We check a known categorical column, e.g., 'job'\n",
    "    with st.expander(\"Processing Details\", expanded=False):\n",
    "        if 'job' in data.columns and data['job'].dtype == 'object':\n",
    "            st.info(\"üîÑ Categorical data detected. Applying pipeline transformations...\")\n",
    "            \n",
    "            # 1. Map Months\n",
    "            month_map = {\n",
    "                'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,\n",
    "                'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,\n",
    "                'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
    "            }\n",
    "            if 'month' in data.columns:\n",
    "                data['month'] = data['month'].map(month_map)\n",
    "    \n",
    "            # 2. Binary Mapping\n",
    "            binary_mapping = {'yes': 1, 'no': 0}\n",
    "            for col in ['deposit', 'default', 'housing', 'loan', 'y']:\n",
    "                if col in data.columns:\n",
    "                    data[col] = data[col].map(binary_mapping)\n",
    "    \n",
    "            # 3. Education Mapping\n",
    "            edu = {'primary': 1, 'secondary': 2, 'tertiary': 3, 'unknown': 0}\n",
    "            if 'education' in data.columns:\n",
    "                data['education_level'] = data['education'].map(edu)\n",
    "            \n",
    "            # 4. One-Hot Encoding\n",
    "            # We must align with the columns the model expects\n",
    "            categorical_cols = ['job', 'marital', 'education', 'contact', 'poutcome']\n",
    "            categorical_cols = [c for c in categorical_cols if c in data.columns]\n",
    "            data = pd.get_dummies(data, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "            \n",
    "            # 5. Handle Missing Columns (Model expects specific columns)\n",
    "            # We need the scaler's expected feature names. \n",
    "            # Since we don't have them easily, we rely on alignment if possible, \n",
    "            # OR usually, we just ensure the user uploads the PRECESSED data.\n",
    "            # But for now, let's assume standard dummy encoding works close enough.\n",
    "            \n",
    "            # Identify target (usually 'deposit' or 'y')\n",
    "            target_options = ['deposit', 'y', 'target']\n",
    "            target_col = next((col for col in target_options if col in data.columns), data.columns[-1])\n",
    "            \n",
    "        else:\n",
    "            st.info(\"Processed data detected.\")\n",
    "            target_col = data.columns[-1]\n",
    "\n",
    "    # --- PREPROCESSING LOGIC END ---\n",
    "\n",
    "    X_test = data.drop(columns=[target_col])\n",
    "    y_test = data[target_col]\n",
    "    y_test = y_test.astype(int)\n",
    "    \n",
    "    # Align columns with scaler (Crucial step often missed)\n",
    "    # If OneHotEncoding produced different columns than training, this will fail.\n",
    "    # Ideally, you should save the 'feature_names' in training and load them here to reindex.\n",
    "    \n",
    "    st.write(f\"Data shape: {data.shape}\")\n",
    "  \n",
    "    try:\n",
    "        # Transform\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "     # Dashboard Layout\n",
    "        st.write(\"Model Accuracy: \", f\"{acc:.2%}\")\n",
    "        st.write(\"**Classification Report:**\")\n",
    "        st.code(classification_report(y_test, y_pred))\n",
    "\n",
    "        st.write(\"**Confusion Matrix**\")\n",
    "        fig, ax = plt.subplots(figsize=(5, 4))\n",
    "        sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        st.pyplot(fig)    \n",
    "   \n",
    "    except ValueError as e:\n",
    "        st.error(f\"Prediction Error: {e}\")\n",
    "        st.info(\"Check if your test CSV matches the training feature set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d6b24-f5ee-465c-aedb-7b2fd14115c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
